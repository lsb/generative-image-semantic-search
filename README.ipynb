{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9aa015f",
   "metadata": {},
   "source": [
    "# 1. Run Stable Diffusion to generate some images\n",
    "# 2. Make a Dataset of these images, and their image embeddings\n",
    "# 3. Semantic image search via Nearest Neighbor text query embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabbdb1",
   "metadata": {},
   "source": [
    "## Motivating example\n",
    "  * there are pictures of \n",
    "    * a chinchilla\n",
    "    * a grizzly\n",
    "    * a bouquet of roses\n",
    "    * a couple smiling at a salad\n",
    "    * a concrete brutalist building\n",
    "  * I want to find all of the pictures closest to \"a fuzzy cute animal\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e8bb5",
   "metadata": {},
   "source": [
    "# 0. Imports and prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892a1451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e8330585764949a177ed1bbecb02c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subdir = \"image-semantic-search\"\n",
    "\n",
    "! mkdir -p {subdir}\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "prompts = [\n",
    "    \"a chinchilla\", \"a grizzly bear\", \"a bouquet of roses\",\n",
    "    \"an ethnically diverse couple smiling at a salad\", \"a concrete brutalist apartment building\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69f5cd",
   "metadata": {},
   "source": [
    "# 1. Generate a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3ea100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98c10b046243318ae2c439e81a041e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6558755c86574d938d1ff7b79b6d96cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3efa704ccf49a5ba250099aa257a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8fa0907fd9414a8a6417d0a475f531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49506947a6e1464bb021bf378d7fcd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for prompt in (prompts * 1):\n",
    "    pipe(prompt).images[0].save(Path(subdir) / f\"{prompt} {datetime.now().timestamp()}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b6e03",
   "metadata": {},
   "source": [
    "# 2. Compile this data set and compute embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef79971",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/lsb/.cache/huggingface/datasets/imagefolder/default-f8063eea6ab80210/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165102e850324df6a2c296aefe232fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af71d60cf72e417cae86dd22a382e095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2708fadcecaa4c1c9aa397975650ba61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/lsb/.cache/huggingface/datasets/imagefolder/default-f8063eea6ab80210/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec737b51dab4467587507f40d34c5ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgfold = load_dataset(\"imagefolder\", data_dir=Path(subdir))['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543b0a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgfold = imgfold.map(lambda e: {\n",
    "    \"name\": Path(e['image'].filename).name,\n",
    "    \"embedding\": model.get_image_features(**processor(images=e['image'], return_tensors=\"pt\", padding=True))[0].detach().numpy(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1a84d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9128b6fa02ef44089553c8e1f1b54c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'name', 'embedding'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgfold.add_faiss_index(column=\"embedding\", index_name=\"flat\", string_factory=\"Flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccee1a8",
   "metadata": {},
   "source": [
    "# 3. Nearest Neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943b4e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a concrete brutalist apartment building 1679299607.33792.jpg',\n",
       " 'a chinchilla 1679298989.838047.jpg',\n",
       " 'an ethnically diverse couple smiling at a salad 1679299455.227876.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgfold.get_nearest_examples(\n",
    "    \"flat\",\n",
    "    model.get_text_features(**tokenizer(\"high-rise architecture\", padding=True, return_tensors=\"pt\"))[0].detach().numpy(),\n",
    "    k=3,\n",
    ").examples['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5868beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a chinchilla 1679298989.838047.jpg',\n",
       " 'a concrete brutalist apartment building 1679299607.33792.jpg',\n",
       " 'a bouquet of roses 1679299300.107625.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgfold.get_nearest_examples(\n",
    "    \"flat\",\n",
    "    model.get_text_features(**tokenizer(\"fuzzy, cute\", padding=True, return_tensors=\"pt\"))[0].detach().numpy(),\n",
    "    k=3,\n",
    ").examples['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6a8896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a bouquet of roses 1679299300.107625.jpg',\n",
       " 'an ethnically diverse couple smiling at a salad 1679299455.227876.jpg',\n",
       " 'a concrete brutalist apartment building 1679299607.33792.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgfold.get_nearest_examples(\n",
    "    \"flat\",\n",
    "    model.get_text_features(**tokenizer(\"flower arrangement\", padding=True, return_tensors=\"pt\"))[0].detach().numpy(),\n",
    "    k=3,\n",
    ").examples['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5665ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
